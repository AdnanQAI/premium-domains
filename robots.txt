
# Robots.txt for Domain2Cart.com
# Last updated: January 1, 2025

# Allow all search engines to crawl the entire site
User-agent: *
Allow: /

# Sitemap location
Sitemap: https://domain2cart.com/sitemap.xml

# Crawl delay (optional - helps prevent server overload)
Crawl-delay: 1

# Block access to sensitive or unnecessary files/folders
Disallow: /admin/
Disallow: /private/
Disallow: /temp/
Disallow: /backup/
Disallow: /.git/
Disallow: /logs/
Disallow: /config/
Disallow: /database/

# Block access to file types that shouldn't be indexed
Disallow: /*.pdf$
Disallow: /*.doc$
Disallow: /*.docx$
Disallow: /*.xls$
Disallow: /*.xlsx$
Disallow: /*.zip$
Disallow: /*.rar$

# Allow access to important resources
Allow: /css/
Allow: /js/
Allow: /images/
Allow: /fonts/
Allow: /assets/

# Specific rules for major search engines

# Google
User-agent: Googlebot
Allow: /
Crawl-delay: 1

# Bing
User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Yahoo
User-agent: Slurp
Allow: /
Crawl-delay: 2

# Yandex
User-agent: YandexBot
Allow: /
Crawl-delay: 2

# Block bad bots and scrapers
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: BLEXBot
Disallow: /

# Block AI training bots (optional)
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

# Additional sitemap references (if you create category-specific sitemaps later)
# Sitemap: https://domain2cart.com/sitemap-domains.xml
# Sitemap: https://domain2cart.com/sitemap-blog.xml
